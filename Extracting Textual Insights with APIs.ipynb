{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Extracting Data from an API using the Requests Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://api.github.com/repositories',\n",
    "                        headers = {'Accept': 'application/vnd.github.v3.full+json'})\n",
    "\n",
    "print(response.status_code) #response code 200 shows it was successful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "application/json; charset=utf-8\n",
      "github.com\n"
     ]
    }
   ],
   "source": [
    "#inspecting the response is another\n",
    "#way to ensure that you parse the response accurately:\n",
    "\n",
    "print(response.encoding)\n",
    "print(response.headers['Content-Type'])\n",
    "print(response.headers['Server'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the response parameters, we understand that the response follows a\n",
    "UTF-8 encoding, and the content is returned using the JSON format\n",
    "\n",
    "Since we already know that the response is a JSON object, we can also\n",
    "use the json() command to read the response. This creates a list object where each\n",
    "element is a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1,\n",
      "  \"node_id\": \"MDEwOlJlcG9zaXRvcnkx\",\n",
      "  \"name\": \"grit\",\n",
      "  \"full_name\": \"mojombo/grit\",\n",
      "  \"private\": false,\n",
      "  \"owner\": {\n",
      "    \"login\": \"mojombo\",\n",
      "    \"id\": 1,\n",
      "    \"node_id\": \"MDQ6VXNlcjE=\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(response.json()[0], indent=2)[:200])  #limited output to the 200 chars in the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "#the previous list of repos is not helpful when looking for specific programming languages or topics\n",
    "#use the Search API:\n",
    "\n",
    "response = requests.get('https://api.github.com/search/repositories')\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request was correct but Server was unable to process the request(422), due to lack of a search query parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.github.com/search/repositories',\n",
    "                       params={'q': 'data_science+language:python'},\n",
    "                       headers = {'Accept':'application/vnd.github.v3.text-match+json'}) \n",
    "#In the Accept parameter we specify text-match+json so tha the response contains the matching metadata and provides response in JSON format \n",
    "\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**awesome**: repositorydescription-\"*Awesome resources on Bioinformatics, data science, machine learning, programming language (Python, Golang, R, Perl) and miscellaneous stuff.*\" matched with***data science***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Python**: repositorydescription-\"*this resporatory have ml,ai,nlp,data science etc.python language related material from many websites eg. datacamp,geeksforgeeks,linkedin,youtube,udemy etc. also it include programming challange/competion solutions*\" matched with***data science***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**hu-dsf**: repositorydescription-\"*Introduction course to data science using the Python programming language in the form of Jupyter Notebooks*\" matched with***data science***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**math-server-docker**: repositorydescription-\"*The ideal multi-user Data Science server with Jupyterhub and RStudio, ready for Python, R and Julia languages.*\" matched with***Data Science***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**python**: repositorydescription-\"*A short course introducing students to the Python programming language for data science*\" matched with***Python***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#list the name of the top 5 repos returned by the search:\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "for item in response.json()['items'][:5]:\n",
    "    printmd('**' + item['name'] + '**' + ': repository' +\n",
    "            item['text_matches'][0]['property']+ '-\\\"*' +\n",
    "            item['text_matches'][0]['fragment']+ '*\\\" matched with'+ '***'+\n",
    "                                 item['text_matches'][0]['matches'][0]['text'] + '***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a use case where we want to monitor the comments in a repo and ensure they adhere to community guidelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Code 200\n",
      "Number of comments 30\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\n",
    "print('Response Code', response.status_code)\n",
    "print('Number of comments', len(response.json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 30 comments from such a popular repo with a lot of collaborators and users, it shows we are missing something;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pagination\n",
    "\n",
    "    > This is a technique used by APIs to limit the number of elements in the response, i.e. one page at a time, for Github, each page contains 30 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next': {'url': 'https://api.github.com/repositories/65600975/issues/comments?page=2',\n",
       "  'rel': 'next'},\n",
       " 'last': {'url': 'https://api.github.com/repositories/65600975/issues/comments?page=1000',\n",
       "  'rel': 'last'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The links field in the response object\n",
    "#provides details on the number of pages in the response\n",
    "\n",
    "response.links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next field provides us with a URL to the next page, which would contain the next\n",
    "30 results, while the last field provides a link to the last page, which provides an\n",
    "indication of how many search results there are in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>2346634237</td>\n",
       "      <td>2024-09-12T15:38:34Z</td>\n",
       "      <td>@pytorchbot rebase -b main</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id            created_at                        body\n",
       "904  2346634237  2024-09-12T15:38:34Z  @pytorchbot rebase -b main"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get all the results we must implement a function that will parse all the rsults on one page\n",
    "#and then call the next URL until the last page has been reached. This is implemented as a \n",
    "#recursive function:\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "def get_all_pages(url, params=None, headers=None):\n",
    "    output_json = []\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        output_json = response.json()\n",
    "        if 'next' in response.links:\n",
    "            next_url = response.links['next']['url']\n",
    "            if next_url is not None:\n",
    "                output_json += get_all_pages(next_url, params, headers)\n",
    "    return output_json\n",
    "\n",
    "out = get_all_pages(\n",
    "    'https://api.github.com/repos/pytorch/pytorch/issues/comments',\n",
    "    params={\n",
    "        'since': '2020-07-01T10:00:01Z',  #filter param of time in ISO 8601 format\n",
    "        'sorted': 'created',\n",
    "        'direction': 'desc'\n",
    "    },\n",
    "    headers = {'Accept': 'application/vnd.github.v3+json'})\n",
    "\n",
    "df = pd.DataFrame(out)\n",
    "\n",
    "print (df['body'].count())\n",
    "df[['id','created_at','body']].sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we have created here can be used to apply text analytics\n",
    "blueprints, for example, to identify comments that do not adhere to community\n",
    "guidelines and flag for moderation. \n",
    "\n",
    "It can also be augmented by running it at programmed\n",
    "time intervals to ensure that latest comments are always captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate Limiting\n",
    "\n",
    "    > This is a technique used by APIs to limit the number of requests that can be made to an API, ina certain timeframe. This is done by using the rate limit headers.\n",
    "\n",
    "We can make a call to the API to only retrieve the headers by using the head\n",
    "method and then peering into the X-Ratelimit-Limit, X-Ratelimit-Remaining, and\n",
    "X-RateLimit-Reset header elements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Ratelimit-Limit 60\n",
      "X-Ratelimit-Remaining 3\n",
      "Rate Limits reset at Fri Sep 13 20:02:28 2024\n"
     ]
    }
   ],
   "source": [
    "response = requests.head(\n",
    "    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\n",
    "\n",
    "print('X-Ratelimit-Limit', response.headers['X-Ratelimit-Limit'])\n",
    "print('X-Ratelimit-Remaining', response.headers['X-Ratelimit-Remaining'])\n",
    "\n",
    "#convert UTC time to human-readable format\n",
    "\n",
    "import datetime\n",
    "\n",
    "print(\n",
    "    'Rate Limits reset at',\n",
    "    datetime.datetime.fromtimestamp(int(\n",
    "        response.headers['X-RateLimit-Reset'])).strftime('%c'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function han\n",
    "dle_rate_limits shown next slows down the requests to ensure they are spaced out\n",
    "over the entire duration. It does so by distributing the remaining requests equally\n",
    "over the remaining time by applying a sleep function.\n",
    "\n",
    "This ensures that our data\n",
    "extraction blueprint respects the rate limits and spaces the requests so that all the\n",
    "requested data is downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time \n",
    "\n",
    "def handle_rate_limits(response):\n",
    "    now = datetime.now()\n",
    "    reset_time = datetime.fromtimestamp(\n",
    "        int(response.headers['X-Ratelimit-Reset']))\n",
    "    remaining_requests = response.header['X-Ratelimit_Remaining']\n",
    "    remaining_time = (reset_time - now).total_seconds()\n",
    "    intervals = remaining_time/ (1.0 + int(remaining_requests))\n",
    "    print('Sleeping for', intervals)\n",
    "    time.sleep(intervals)\n",
    "    return True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retry strategy will allow API\n",
    "calls to be retried in case of specified failure conditions. It can be implemented\n",
    "with the HTTPAdapter library that allows more fine-grained control of the underlying\n",
    "HTTP connections being made.\n",
    "\n",
    "-Specify a backoff_factor value to increase the delay between each retry, and thus avoid hammering the server. Specify a custom adapter that allows you to implement the retry strategy, by overriding the connection behavior of the default Sessions object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome\n",
      "Python\n",
      "hu-dsf\n",
      "math-server-docker\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    \n",
    "    total=5,  #retry 5 times for a failed attempt/request\n",
    "    status_forcelist = [500, 503, 504], #retry on these specific server errors\n",
    "    backoff_factor = 1 #time delay between retries\n",
    ")\n",
    "\n",
    "retry_adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "http = requests.Session()\n",
    "http.mount('https://', retry_adapter)  #custom retry adapter\n",
    "http.mount('http://', retry_adapter) \n",
    "\n",
    "response = http.get('https://api.github.com/search/repositories',\n",
    "                    params={'q': 'data_science+language:python'})\n",
    "\n",
    "for item in response.json()['items'][:5]:\n",
    "    print(item['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all this together, we can modify our blueprint to handle pagination, rate limits and retries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total =5,\n",
    "    status_forcelist=[500,503,504],\n",
    "    backoff_factor = 1\n",
    ")\n",
    "\n",
    "retry_adapter = HTTPAdapter(max_retries = retry_strategy)\n",
    "\n",
    "http = requests.Session()\n",
    "http.mount('https://', retry_adapter)\n",
    "http.mount('http://', retry_adapter)\n",
    "\n",
    "def get_all_pages(url, param=None, header=None):\n",
    "    output_json = []\n",
    "    response = http.get(url, params=param, headers=header)\n",
    "    if response.status_code == 200:\n",
    "        output_json = response.json()\n",
    "        if 'next' in response.links:\n",
    "            next_url = response.links['next']['url']\n",
    "            if (next_url is not None) and (handle_rate_limits(response)):\n",
    "                output_json += get_all_pages(next_url, param, header)\n",
    "\n",
    "    return output_json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Unauthenticated requests have a lower rate limit. You can ID your data extraction app to Github by registering an account, then make authenticated requests to the API that increases the rate limit.\n",
    "\n",
    "The blueprint aboce shows how to extract data from any API using the simple requests module, and creating your own dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Extracting Twitter Data with Tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing and Configuring Tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to authenticate the app with the Twitter API,\n",
    "and we do it with the help of the tweepy.AppAuthHandler module to which we pass\n",
    "the API key and API secret key we obtained in the previous step(documentation). \n",
    "\n",
    "Finally, we instantiate\n",
    "the tweepy.API class, which will be used to make all subsequent calls to the Twitter\n",
    "API. Once the connection is made, we can confirm the host and version of the\n",
    "API object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: <tweepy.client.Client object at 0x00000233EDA53EC0>\n",
      "4.14.0\n"
     ]
    }
   ],
   "source": [
    "import tweepy \n",
    "\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAAxrvwEAAAAAIIloa2%2FF9gZIbf3nRUgGBjaD3tc%3DvT47j9XLLH99dpoVUFWI8L8mhBa3M5vwvqvYcDW3XkxdychNIo'\n",
    "\n",
    "client = tweepy.Client(bearer_token)\n",
    "\n",
    "\n",
    "print('Client:', client)\n",
    "\n",
    "print(tweepy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Data from the Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: 1778002463053737984\n",
      "Name: Afiadata\n",
      "Username: afiadata_ke\n",
      "Followers Count: 398\n",
      "Following Count: 19\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "username = 'afiadata_ke'\n",
    "\n",
    "try:\n",
    "    user = client.get_user(username=username, user_fields=['public_metrics'])\n",
    "    print(f'User ID: {user.data.id}')\n",
    "    print(f'Name: {user.data.name}')\n",
    "    print(f'Username: {user.data.username}')\n",
    "    if user.data.public_metrics:\n",
    "        print(f'Followers Count: {user.data.public_metrics['followers_count']}')\n",
    "        print(f'Following Count: {user.data.public_metrics['following_count']}')\n",
    "    else:\n",
    "        print('Public metrics are not available.')\n",
    "except tweepy.TweepyException as e:\n",
    "    #handling rate limit when exceeded \n",
    "\n",
    "    reset_time = e.retry_after if hasattr(e, 'retry_after') else 60\n",
    "    print(f'Rate limit exceeded. Waiting for {reset_time} seconds')\n",
    "    time.sleep(reset_time)\n",
    "    fetch_user(username)  #retry the request after waiting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter has an access barrier, such that the below code can only run for premium accounts, when fetching or retrieving tweets. Please check X (Twitter) API key documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_term = 'afiadata'\n",
    "\n",
    "#tweets = tweepy.Cursor(api.search_tweets, q = search_term, lang='en').items(100)\n",
    "\n",
    "#retrieved_tweets = [tweet._json for tweet in tweets] #load the results as a  json object and return a list\n",
    "#df = pd.json_normalize(retrieved_tweets)  #return a dataframe from the json object\n",
    "\n",
    "#df[['text']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we wanted to filter by RTs, and retrieve the full text of all tweets. The Standard search API seraches only a sample of recent tweets publshed in the past 7 days. Count is the number of tweets that can be retrieved in one call. \n",
    "\n",
    "Note: This is an alternative code to use once on premium access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#api = tweepy.API(auth,\n",
    "#                wait_on_rate_limit=True,\n",
    "#                wait_on_rate_limit_notify=True,\n",
    "#                retry_count=5,\n",
    "#                retry_delay=10)\n",
    "\n",
    "#search_term ='afiadata_ke OR afiadata - filter:retweets'\n",
    "\n",
    "#tweets = tweepy.Cursor(api.search_tweets, q = search_term,\n",
    "#                        lang='en',\n",
    "#                       tweet_mode='extended',\n",
    "#                       count=30).items(12000)  #limit to 12000 tweets. We can retrieve 30 per call\n",
    "\n",
    "#retrieved_tweets = [tweet._json for tweet in tweets] #load the results as a  json object and return a list\n",
    "\n",
    "#df = pd.json_normalize(retrieved_tweets)\n",
    "#print('Number of retrieved tweets', len(df))\n",
    "\n",
    "#df[['created_at','full_text','entities.hashtags']].sample(2) #extract the relevant fields \n",
    "\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter also returns several entities such\n",
    "as hashtags contained within the tweet, and it would be interesting to see which hashtags\n",
    "are used heavily when discussing afiadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "#def extract_entities(entity_list):\n",
    "#    entities = set()\n",
    "#    if len(entity_list) != 0:\n",
    "#        for item in entity_list:\n",
    "#            for key,value in item.items():\n",
    "#                if key== 'text':\n",
    "#                    entities.add(value.lower())\n",
    "#    return list(entities)\n",
    "\n",
    "#df['Entities'] = df['entities.hashtags'].apply(extract_entities)\n",
    "#pd.Series(np.concatenate(df['Entities'])).value_counts()[:25].plot(kind='barh', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Data from a User's Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Please note the acess level your API has to the Twitter API, to determine which classes to use under tweepy. Some methods apply to premium accounts only and therefore do not serve a purpose for free accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming you were on Premium Access: (otherwise you cannot retrieve without premium access)\n",
    "\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "#tweets = tweepy.Cursor(api.user_timeline,\n",
    "#                       screen_name = 'afiadata_ke',\n",
    "#                       lang='en',\n",
    "#                       tweet_mode='extended',\n",
    "#                       count =50).items(200)\n",
    "#retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "#df = pd.io.json.json_normalize(retrieved_tweets) \n",
    "#print('Number of retrieved tweets', len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_user_timeline(screen_name):  #screen name is the handle\n",
    "#    api = tweepy.API(auth,\n",
    "#                     wait_on_rate_limit=True,\n",
    "#                     wait_on_rate_limit_notify=True)\n",
    "#    tweets = tweepy.Cursor(api.user_timeline,\n",
    "#                           screen_name=screen_name,\n",
    "#                           lang=\"en\",\n",
    "#                           tweet_mode='extended',\n",
    "#                           count=200).items()\n",
    "#    retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "#    df = pd.io.json.json_normalize(retrieved_tweets)\n",
    "#    df = df[~df['retweeted_status.id'].isna()]\n",
    "#    return df\n",
    "\n",
    "#df = get_user_timeline('afiadata_ke')\n",
    "#print('Number of retrieved tweets', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the quirks of the Tweepy implementation is that in the case\n",
    "of retweets, the full_text column is truncated, and the retweeted_status.full_text column must be used to retrieve all the\n",
    "characters of the tweet. \n",
    "\n",
    "For our use case, retweets are not important,\n",
    "and we filter them by checking if retweeted_status.id is\n",
    "empty. However, depending on the use case, you can add a condition\n",
    "to replace the column full_text with retweeted_status.full_text in the case of retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a wordcloud as learnt earlier, or import the wordcloud function from the previous chapter file and identify the keywords being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Data from the Streaming API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math \n",
    "\n",
    "bearer_token = bearer_token\n",
    "\n",
    "access_token = '555015082-3aHs9R2YlpqrFJLuuFyaPpyUptBaJP7L0VVWm3pV'\n",
    "\n",
    "access_token_secret = 'FFEmTmkq3vNPJwCzBhKjzQaim758Wpip0lupSv8A59Ev1'\n",
    "\n",
    "consumer_key = 'KVUHB4pZsxxYe8DHH72yI9ZfU'\n",
    "\n",
    "consumer_secret = 'PGUBmcOzX1vofA467qJ1PYXf4vmwKM0FcnggoRyIciq7viRc58' \n",
    "\n",
    "\n",
    "\n",
    "class MyStream(tweepy.StreamingClient):\n",
    "    def __init__(self, bearer_token, max_tweets=math.inf):\n",
    "        super().__init__(bearer_token)\n",
    "        self.num_tweets = 0\n",
    "        self.max_tweets = max_tweets\n",
    "\n",
    "    def on_tweet(self, tweet): #method is called when a tweet is received\n",
    "        if self.num_tweets < self.max_tweets:\n",
    "            print(f'{datetime.now()}: {tweet.text}')\n",
    "            self.num_tweets +=1\n",
    "\n",
    "        else:\n",
    "            self.disconnect()  #quit streaming if max_tweets is reached\n",
    "\n",
    "    def on_errors(self, status_code):\n",
    "        if status_code == 420:\n",
    "            print('Rate limit reached. Waiting...')\n",
    "        else:\n",
    "            print(f'Error received: {status_code}')\n",
    "        return True #continues listening to stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stream encountered HTTP error: 403\n",
      "HTTP error response text: {\"client_id\":\"29321996\",\"detail\":\"When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\",\"registration_url\":\"https://developer.twitter.com/en/docs/projects/overview\",\"title\":\"Client Forbidden\",\"required_enrollment\":\"Appropriate Level of API Access\",\"reason\":\"client-not-enrolled\",\"type\":\"https://api.twitter.com/2/problems/client-forbidden\"}\n",
      "Stream encountered HTTP error: 403\n",
      "HTTP error response text: {\"client_id\":\"29321996\",\"detail\":\"When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\",\"registration_url\":\"https://developer.twitter.com/en/docs/projects/overview\",\"title\":\"Client Forbidden\",\"required_enrollment\":\"Appropriate Level of API Access\",\"reason\":\"client-not-enrolled\",\"type\":\"https://api.twitter.com/2/problems/client-forbidden\"}\n",
      "Stream encountered HTTP error: 403\n",
      "HTTP error response text: {\"client_id\":\"29321996\",\"detail\":\"When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\",\"registration_url\":\"https://developer.twitter.com/en/docs/projects/overview\",\"title\":\"Client Forbidden\",\"required_enrollment\":\"Appropriate Level of API Access\",\"reason\":\"client-not-enrolled\",\"type\":\"https://api.twitter.com/2/problems/client-forbidden\"}\n"
     ]
    }
   ],
   "source": [
    "##provide user authentication tokens and initialize the stream\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "api = tweepy. API(auth)\n",
    "\n",
    "stream = MyStream(bearer_token=bearer_token, max_tweets=50)\n",
    "\n",
    "#add a rule for filtering tweets \n",
    "#stream.add_rules(tweepy.StreamRule('Python')) --> this works only on premium, upgrade your package to run the code\n",
    "\n",
    "stream.filter(tweet_fields = ['Context_annnotations', 'created_at'])\n",
    "\n",
    "#upgrade package to basic or premium to access these features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 403 Forbidden\n",
      "453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tweepy\n",
    "\n",
    "# Your keys and tokens\n",
    "bearer_token = bearer_token\n",
    "\n",
    "access_token = '555015082-3aHs9R2YlpqrFJLuuFyaPpyUptBaJP7L0VVWm3pV'\n",
    "\n",
    "access_token_secret = 'FFEmTmkq3vNPJwCzBhKjzQaim758Wpip0lupSv8A59Ev1'\n",
    "\n",
    "consumer_key = 'KVUHB4pZsxxYe8DHH72yI9ZfU'\n",
    "\n",
    "consumer_secret = 'PGUBmcOzX1vofA467qJ1PYXf4vmwKM0FcnggoRyIciq7viRc58' \n",
    "\n",
    "# Authenticate to Twitter using OAuth1\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Post a tweet\n",
    "try:\n",
    "    api.update_status(\"First tweet from our API!\")\n",
    "    print(\"Tweet posted successfully!\")\n",
    "except tweepy.TweepyException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "#upgrade package to basic from free plan \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a publicly available data source such as wikipedia, the authentication and generation of access tokens is not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'NLPTraining/1.0(dwayneqlint1@gmail.com)' \n",
    "\n",
    "#user agent is needed, and typically includes, name of app, a versionnumber and a way to contact the developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work as a medium of exchange through a computer network that is not reliant on any central authority, such as a governmen ...\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi \n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language = 'en',\n",
    "    extract_format = wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page('Cryptocurrency')\n",
    "print(p_wiki.text[:200], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
