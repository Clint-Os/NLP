{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Websites and Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Downloading and Interpreting robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "rp= urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(\"https://web.pharmacyboardkenya.org/robots.txt\")\n",
    "rp.read()\n",
    "rp.can_fetch(\"*\", \"https://web.pharmacyboardkenya.org/sitemap.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Finding URLs from sitemap.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict #this converts the sitemap.xml file to a dictionary\n",
    "import requests \n",
    "\n",
    "sitemap = xmltodict.parse(requests.get(\"https://web.pharmacyboardkenya.org/sitemap.xml\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sitemapindex': {'@xmlns': 'http://www.sitemaps.org/schemas/sitemap/0.9', 'sitemap': [{'loc': 'https://web.pharmacyboardkenya.org/post-sitemap.xml', 'lastmod': '2024-09-15T18:51:07+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/page-sitemap.xml', 'lastmod': '2024-08-27T10:23:56+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/wpdmpro-sitemap.xml', 'lastmod': '2024-08-16T12:40:36+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/ot_header_builders-sitemap.xml', 'lastmod': '2024-04-12T10:22:07+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/ot_footer_builders-sitemap.xml', 'lastmod': '2024-03-08T09:52:24+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/post-archive-sitemap.xml', 'lastmod': '2024-08-16T12:40:36+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/category-sitemap.xml', 'lastmod': '2024-09-15T18:51:07+00:00'}, {'loc': 'https://web.pharmacyboardkenya.org/wpdmcategory-sitemap.xml', 'lastmod': '2024-08-16T12:40:36+00:00'}]}}\n"
     ]
    }
   ],
   "source": [
    "print(sitemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected sitemap structure: dict_keys(['sitemapindex'])\n"
     ]
    }
   ],
   "source": [
    "#check what is in the dict before downloading the files:\n",
    "\n",
    "if 'urlset' in sitemap and 'url' in sitemap['urlset']:\n",
    "    urls = [url['loc'] for url in sitemap['urlset']['url']]\n",
    "else:\n",
    "    print(\"Unexpected sitemap structure:\", sitemap.keys())\n",
    "\n",
    "\n",
    "#this gives us an idea of our sitemap structure that is used in the website. Instead of\n",
    "# 'urlset', our uses sitemap index first as shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_urls = [sitemap['loc'] for sitemap in sitemap['sitemapindex']['sitemap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://web.pharmacyboardkenya.org/blog-2/\n",
      "https://web.pharmacyboardkenya.org/%f0%9d%90%8f%f0%9d%90%8f%f0%9d%90%81-%f0%9d%90%9a%f0%9d%90%a7%f0%9d%90%9d-%f0%9d%90%8a%f0%9d%90%80%f0%9d%90%8f%f0%9d%90%88-%f0%9d%90%8c%f0%9d%90%9a%f0%9d%90%ab%f0%9d%90%a4-%f0%9d%90%96%f0%9d%90%a8/\n",
      "https://web.pharmacyboardkenya.org/%f0%9d%90%80%f0%9d%90%94-%f0%9d%9f%91%f0%9d%90%92-%f0%9d%90%92%f0%9d%90%ad%f0%9d%90%9e%f0%9d%90%9e%f0%9d%90%ab%f0%9d%90%a2%f0%9d%90%a7%f0%9d%90%a0-%f0%9d%90%82%f0%9d%90%a8%f0%9d%90%a6%f0%9d%90%a6/\n"
     ]
    }
   ],
   "source": [
    "def fetch_sitemap(url):\n",
    "    response = requests.get(url)\n",
    "    return xmltodict.parse(response.content)\n",
    "\n",
    "all_urls = []\n",
    "\n",
    "for sitemap_url in sitemap_urls:\n",
    "    individual_sitemap = fetch_sitemap(sitemap_url)\n",
    "    if 'urlset' in individual_sitemap and 'url' in individual_sitemap['urlset']:\n",
    "        urls = [url['loc'] for url in individual_sitemap['urlset']['url']]\n",
    "        all_urls.extend(urls) \n",
    "    else:\n",
    "        print(f'Unexpected structure is sitemap {sitemap_url}')\n",
    "\n",
    "print(\"\\n\".join(all_urls[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Finding URLs from RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often (and\n",
    "sometimes more easily) this can be found by taking a look at the source code of the\n",
    "corresponding webpage and searching for RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğğğ ğšğ§ğ ğŠğ€ğğˆ ğŒğšğ«ğ¤ ğ–ğ¨ğ«ğ¥ğ ğğšğ­ğ¢ğğ§ğ­ ğ’ğšğŸğğ­ğ² ğƒğšğ² ğ°ğ¢ğ­ğ¡ ğ…ğ¨ğœğ®ğ¬ ğ¨ğ§ ğˆğ¦ğ©ğ«ğ¨ğ¯ğ¢ğ§ğ  ğƒğ¢ğšğ ğ§ğ¨ğ¬ğ¢ğ¬ â€“ 13th September 2024',\n",
       "  'https://web.pharmacyboardkenya.org/%f0%9d%90%8f%f0%9d%90%8f%f0%9d%90%81-%f0%9d%90%9a%f0%9d%90%a7%f0%9d%90%9d-%f0%9d%90%8a%f0%9d%90%80%f0%9d%90%8f%f0%9d%90%88-%f0%9d%90%8c%f0%9d%90%9a%f0%9d%90%ab%f0%9d%90%a4-%f0%9d%90%96%f0%9d%90%a8/?utm_source=rss&utm_medium=rss&utm_campaign=%25f0%259d%2590%258f%25f0%259d%2590%258f%25f0%259d%2590%2581-%25f0%259d%2590%259a%25f0%259d%2590%25a7%25f0%259d%2590%259d-%25f0%259d%2590%258a%25f0%259d%2590%2580%25f0%259d%2590%258f%25f0%259d%2590%2588-%25f0%259d%2590%258c%25f0%259d%2590%259a%25f0%259d%2590%25ab%25f0%259d%2590%25a4-%25f0%259d%2590%2596%25f0%259d%2590%25a8'),\n",
       " ('ğ€ğ”-ğŸ‘ğ’ ğ’ğ­ğğğ«ğ¢ğ§ğ  ğ‚ğ¨ğ¦ğ¦ğ¢ğ­ğ­ğğ ğŒğğğ­ğ¢ğ§ğ  ğ„ğ§ğğ¬ ğ°ğ¢ğ­ğ¡ ğ‚ğ¨ğ¦ğ¦ğ¢ğ­ğ¦ğğ§ğ­ ğ­ğ¨ ğğ¨ğ¨ğ¬ğ­ ğŒğğğ¢ğœğšğ¥ ğğ«ğ¨ğğ®ğœğ­ ğ’ğšğŸğğ­ğ² ğ¢ğ§ ğ€ğŸğ«ğ¢ğœğš\\xa0 â€“ 13th September 2024',\n",
       "  'https://web.pharmacyboardkenya.org/%f0%9d%90%80%f0%9d%90%94-%f0%9d%9f%91%f0%9d%90%92-%f0%9d%90%92%f0%9d%90%ad%f0%9d%90%9e%f0%9d%90%9e%f0%9d%90%ab%f0%9d%90%a2%f0%9d%90%a7%f0%9d%90%a0-%f0%9d%90%82%f0%9d%90%a8%f0%9d%90%a6%f0%9d%90%a6/?utm_source=rss&utm_medium=rss&utm_campaign=%25f0%259d%2590%2580%25f0%259d%2590%2594-%25f0%259d%259f%2591%25f0%259d%2590%2592-%25f0%259d%2590%2592%25f0%259d%2590%25ad%25f0%259d%2590%259e%25f0%259d%2590%259e%25f0%259d%2590%25ab%25f0%259d%2590%25a2%25f0%259d%2590%25a7%25f0%259d%2590%25a0-%25f0%259d%2590%2582%25f0%259d%2590%25a8%25f0%259d%2590%25a6%25f0%259d%2590%25a6'),\n",
       " ('ğŠğğ§ğ²ğš ğ‡ğ¨ğ¬ğ­ğ¬ ğŸ“ğ­ğ¡ ğ€ğ”-ğŸ‘ğ’ ğ’ğ­ğğğ«ğ¢ğ§ğ  ğ‚ğ¨ğ¦ğ¦ğ¢ğ­ğ­ğğ ğŒğğğ­ğ¢ğ§ğ  ğ­ğ¨ ğ’ğ­ğ«ğğ§ğ ğ­ğ¡ğğ§ ğ‡ğğšğ¥ğ­ğ¡ ğ’ğšğŸğğ­ğ² ğ¢ğ§ ğ€ğŸğ«ğ¢ğœğš â€“ September 12, 2024',\n",
       "  'https://web.pharmacyboardkenya.org/%f0%9d%90%8a%f0%9d%90%9e%f0%9d%90%a7%f0%9d%90%b2%f0%9d%90%9a-%f0%9d%90%87%f0%9d%90%a8%f0%9d%90%ac%f0%9d%90%ad%f0%9d%90%ac-%f0%9d%9f%93%f0%9d%90%ad%f0%9d%90%a1-%f0%9d%90%80%f0%9d%90%94-%f0%9d%9f%91/?utm_source=rss&utm_medium=rss&utm_campaign=%25f0%259d%2590%258a%25f0%259d%2590%259e%25f0%259d%2590%25a7%25f0%259d%2590%25b2%25f0%259d%2590%259a-%25f0%259d%2590%2587%25f0%259d%2590%25a8%25f0%259d%2590%25ac%25f0%259d%2590%25ad%25f0%259d%2590%25ac-%25f0%259d%259f%2593%25f0%259d%2590%25ad%25f0%259d%2590%25a1-%25f0%259d%2590%2580%25f0%259d%2590%2594-%25f0%259d%259f%2591'),\n",
       " ('PRESS RELEASE â€“ BOARD SUSPENDS LICENCES OF FOUR PHARMACEUTICAL PRACTITIONERS_9TH SEPTEMBER 2024',\n",
       "  'https://web.pharmacyboardkenya.org/press-release-board-suspends-licences-of-four-pharmaceutical-practitioners_9th-september-2024/?utm_source=rss&utm_medium=rss&utm_campaign=press-release-board-suspends-licences-of-four-pharmaceutical-practitioners_9th-september-2024'),\n",
       " ('ğğğ ğšğ§ğ ğ”ğ’ğ€ğˆğƒ ğ’ğ­ğ«ğğ§ğ ğ­ğ¡ğğ§ ğ‚ğ¨ğ¥ğ¥ğšğ›ğ¨ğ«ğšğ­ğ¢ğ¨ğ§ ğ­ğ¨ ğ„ğ§ğ¡ğšğ§ğœğ ğŠğğ§ğ²ğšâ€™ğ¬ ğ‡ğğšğ¥ğ­ğ¡ğœğšğ«ğ ğ’ğ­ğšğ§ğğšğ«ğğ¬ ğšğ§ğ ğ‹ğ¨ğœğšğ¥ ğŒğšğ§ğ®ğŸğšğœğ­ğ®ğ«ğ¢ğ§ğ  â€“ 20TH AUGUST 2024',\n",
       "  'https://web.pharmacyboardkenya.org/%f0%9d%90%8f%f0%9d%90%8f%f0%9d%90%81-%f0%9d%90%9a%f0%9d%90%a7%f0%9d%90%9d-%f0%9d%90%94%f0%9d%90%92%f0%9d%90%80%f0%9d%90%88%f0%9d%90%83-%f0%9d%90%92%f0%9d%90%ad%f0%9d%90%ab%f0%9d%90%9e%f0%9d%90%a7/?utm_source=rss&utm_medium=rss&utm_campaign=%25f0%259d%2590%258f%25f0%259d%2590%258f%25f0%259d%2590%2581-%25f0%259d%2590%259a%25f0%259d%2590%25a7%25f0%259d%2590%259d-%25f0%259d%2590%2594%25f0%259d%2590%2592%25f0%259d%2590%2580%25f0%259d%2590%2588%25f0%259d%2590%2583-%25f0%259d%2590%2592%25f0%259d%2590%25ad%25f0%259d%2590%25ab%25f0%259d%2590%259e%25f0%259d%2590%25a7'),\n",
       " ('ğğğ ğ‡ğ¨ğ¬ğ­ğ¬ ğ’ğ­ğ«ğšğ­ğğ ğ¢ğœ ğğšğ«ğ­ğ§ğğ«ğ¬ ğŒğğğ­ğ¢ğ§ğ  ğ¨ğ§ ğŒğšğ§ğğšğ­ğ¨ğ«ğ² ğğ¢ğ¨ğğªğ®ğ¢ğ¯ğšğ¥ğğ§ğœğ ğ‘ğğªğ®ğ¢ğ«ğğ¦ğğ§ğ­ğ¬ ğŸğ¨ğ« ğ‹ğ¨ğœğšğ¥ğ¥ğ² ğŒğšğ§ğ®ğŸğšğœğ­ğ®ğ«ğğ ğğ¡ğšğ«ğ¦ğšğœğğ®ğ­ğ¢ğœğšğ¥ğ¬ â€“ 20TH AUGUST 2024',\n",
       "  'https://web.pharmacyboardkenya.org/%f0%9d%90%8f%f0%9d%90%8f%f0%9d%90%81-%f0%9d%90%87%f0%9d%90%a8%f0%9d%90%ac%f0%9d%90%ad%f0%9d%90%ac-%f0%9d%90%92%f0%9d%90%ad%f0%9d%90%ab%f0%9d%90%9a%f0%9d%90%ad%f0%9d%90%9e%f0%9d%90%a0%f0%9d%90%a2/?utm_source=rss&utm_medium=rss&utm_campaign=%25f0%259d%2590%258f%25f0%259d%2590%258f%25f0%259d%2590%2581-%25f0%259d%2590%2587%25f0%259d%2590%25a8%25f0%259d%2590%25ac%25f0%259d%2590%25ad%25f0%259d%2590%25ac-%25f0%259d%2590%2592%25f0%259d%2590%25ad%25f0%259d%2590%25ab%25f0%259d%2590%259a%25f0%259d%2590%25ad%25f0%259d%2590%259e%25f0%259d%2590%25a0%25f0%259d%2590%25a2'),\n",
       " ('PUBLIC ALERT: VOLUNTARY RECALL OF S-PRAZO (ESOMEPRAZOLE MAGNESIUM DELAYED-RELEASE CAPSULES 40MG) BATCH NO SPZ-302 MANUFACTURED BY LABORATE PHARMACEUTICAL INDIA LIMITED- 20TH AUGUST 2024',\n",
       "  'https://web.pharmacyboardkenya.org/public-alert-voluntary-recall-of-s-prazo-esomeprazole-magnesium-delayed-release-capsules-40mg-batch-no-spz-302-manufactured-by-laborate-pharmaceutical-india-limited-20th-august-2024/?utm_source=rss&utm_medium=rss&utm_campaign=public-alert-voluntary-recall-of-s-prazo-esomeprazole-magnesium-delayed-release-capsules-40mg-batch-no-spz-302-manufactured-by-laborate-pharmaceutical-india-limited-20th-august-2024'),\n",
       " ('Pharmacy and Poisons Board Adopts New Technology to Ensure Drug Quality, 29th July 2024',\n",
       "  'https://web.pharmacyboardkenya.org/pharmacy-and-poisons-board-adopts-new-technology-to-ensure-drug-quality-29th-july-2024/?utm_source=rss&utm_medium=rss&utm_campaign=pharmacy-and-poisons-board-adopts-new-technology-to-ensure-drug-quality-29th-july-2024'),\n",
       " ('Kenya and Malawi Strengthen Pharmacovigilance Partnership, 29th July 2024',\n",
       "  'https://web.pharmacyboardkenya.org/kenya-and-malawi-strengthen-pharmacovigilance-partnership-29th-july-2024/?utm_source=rss&utm_medium=rss&utm_campaign=kenya-and-malawi-strengthen-pharmacovigilance-partnership-29th-july-2024'),\n",
       " ('PUBLIC ALERT ON FALSIFIED OZEMPIC (SEMAGLUTIDE) PENS',\n",
       "  'https://web.pharmacyboardkenya.org/public-alert-on-falsified-ozempic-semaglutide-pens/?utm_source=rss&utm_medium=rss&utm_campaign=public-alert-on-falsified-ozempic-semaglutide-pens')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "feed = feedparser.parse('https://web.pharmacyboardkenya.org/feed/')\n",
    "\n",
    "[(e.title, e.link) for e in feed.entries] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [e.id for e in feed.entries] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://web.pharmacyboardkenya.org/?p=10573',\n",
       " 'https://web.pharmacyboardkenya.org/?p=10570',\n",
       " 'https://web.pharmacyboardkenya.org/?p=10562',\n",
       " 'https://web.pharmacyboardkenya.org/?p=10558',\n",
       " 'https://web.pharmacyboardkenya.org/?p=10164',\n",
       " 'https://web.pharmacyboardkenya.org/?p=10158',\n",
       " 'https://web.pharmacyboardkenya.org/?p=10155',\n",
       " 'https://web.pharmacyboardkenya.org/?p=9958',\n",
       " 'https://web.pharmacyboardkenya.org/?p=9955',\n",
       " 'https://web.pharmacyboardkenya.org/?p=9946']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an alternative way to get a list of URLs when no sitemap.xml is available. Atom feeds offer the same information as RSS in a different format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Downloading HTML pages with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 14.15 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import re \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "#func to sanitize urls\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[^\\w\\-_\\.]','_', filename)\n",
    "\n",
    "for url in urls[0:10]:\n",
    "    #get the part after the last / in the url and use as a filename\n",
    "    filename = url.rsplit('/')[-1]\n",
    "    filename = sanitize_filename(filename)\n",
    "\n",
    "    r = s.get(url)\n",
    "    if r.ok:\n",
    "        with open(filename, 'w+b') as f:\n",
    "            f.write(r.text.encode('utf-8'))\n",
    "    else:\n",
    "        print(f'error with URL: {url}')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f'execution time: {execution_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Downloading HTML Pages with wget(a command line tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wget supports lists of URLs for downloads and HTTP keep-alive. \n",
    "\n",
    "The -nc option of wget will check\n",
    "whether files have already been downloaded. This way, we can avoid downloading\n",
    "content twice. We can now stop the process at any time and restart without losing\n",
    "data, which is important if a web server blocks us, our Internet connection goes\n",
    "down,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.txt','w+b') as f:\n",
    "    f.write('\\n'.join(urls).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to your command line or open a terminal on Jupyter. Then run the following command:\n",
    "\n",
    "wget -nc -i urls.txt\n",
    "\n",
    "Make sure to install wget and add it to your system path, then read documentation if any problems arise. Ensure to disable the inbuilt wget alias and specify the installed wget path if any issue arises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i option tells wget the lit of urls to downlaod. wget skips the exisitng file due to the -nc option(it means no clobber). wget can also be used for recursively downloading websites with the option -r. Combine it with -l to specify the depth/level of the recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Semistructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Extracting Data with Regular Expressions (we already did this in the code above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import re \n",
    "\n",
    "def sanitize_file(file):\n",
    "    return re.sub(r'[^\\w\\-_\\.]','_', file)\n",
    "\n",
    "url = 'https://web.pharmacyboardkenya.org/pharmacy-and-poisons-board-adopts-new-technology-to-ensure-drug-quality-29th-july-2024/?utm_source=rss&utm_medium=rss&utm_campaign=pharmacy-and-poisons-board-adopts-new-technology-to-ensure-drug-quality-29th-july-2024'\n",
    "\n",
    "s= requests.Session()\n",
    "#use the part after the lat / as the filename\n",
    "file = url.split('/')[-1] + '.html'\n",
    "file = sanitize_file(file)\n",
    "r = s.get(url)\n",
    "\n",
    "if r.ok:\n",
    "    with open(file, 'w+b') as f:\n",
    "        f.write(r.text.encode('utf-8'))\n",
    "\n",
    "else:\n",
    "    print(f'Error with URL: {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pharmacy and Poisons Board Adopts New Technology to Ensure Drug Quality, 29th July 2024 - Pharmacy and Poisons Board\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    html=f.read()\n",
    "    g = re.search(r'<title>(.*)</title>', html, re.MULTILINE | re.DOTALL)\n",
    "    if g:\n",
    "        print(g.groups()[0])  #prints the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re library is not fully integrated into Python string handling thus cannot be invoked as methods of string.\n",
    "\n",
    "As our HTML documents consist of many\n",
    "lines, we have to use re.MULTILINE|re.DOTALL. Sometimes cascaded calls to\n",
    "re.search are necessary, thought they make the code harder to read.\n",
    "\n",
    "Use re.search and not re.match in Py. The latter tries to match the whole string, and\n",
    "as there is data before < title> and after </ title>, it fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Using an HTML Parser for Extraction (using Beautiful Soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting the title/headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h4 class=\"entry-title\">Pharmacy and Poisons Board Adopts New Technology to Ensure Drug Quality, 29th July 2024</h4>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup.select('h4.entry-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h4 class=\"entry-title\">Pharmacy and Poisons Board Adopts New Technology to Ensure Drug Quality, 29th July 2024</h4>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the tag names directly \n",
    "soup.h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pharmacy and Poisons Board Adopts New Technology to Ensure Drug Quality, 29th July 2024'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract text without the HTML clutter around it \n",
    "soup.h4.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in contrast to the regular expression solution, unnecessary whitespaces\n",
    "have been stripped by Beautiful Soup.\n",
    "Unfortunately, that does not work as well for the title, see the -ppb suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pharmacy and Poisons Board Adopts New Technology to Ensure Drug Quality, 29th July 2024 - Pharmacy and Poisons Board'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting the article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\xa0\\n\\xa0 \\nKenya â€“ The Pharmacy and Poisons Board (PPB) has acquired advanced Near Infrared (NIR) technology, the Pillscan, to strengthen its efforts in safeguarding drug quality. Provided by the Mission of Essential Medicines and Supplies, this new tool will be used for on-site screening of medical products at PPB regional offices and key entry points.\\nComplementing the PPBâ€™s existing quality control measures, the NIR technology will enhance the detection of sub-standard and falsified medical products. To maximize the impact of this new tool, a six-day training was conducted for PPB staff, county pharmacists, and KEMSA personnel.\\nBy incorporating NIR technology into its operations, the PPB aims to bolster its surveillance of the Kenyan pharmaceutical market and protect public health.\\nThe Pharmacy and Poisons Board commends the Global Fund for supporting its mission to ensure safe and quality medicines for Kenyans. This investment in advanced technology is a significant step forward in ensuring the quality and safety of medicines accessible to the public.\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one('div.entry-summary').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"\" class=\"alignnone size-medium wp-image-9960\" decoding=\"async\" fetchpriority=\"high\" height=\"225\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54-300x225.jpg\" srcset=\"https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54-300x225.jpg 300w, https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54.jpg 640w\" width=\"300\"/>,\n",
       " <img alt=\"\" class=\"alignnone size-medium wp-image-9961\" decoding=\"async\" height=\"300\" sizes=\"(max-width: 225px) 100vw, 225px\" src=\"https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-53-225x300.jpg\" srcset=\"https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-53-225x300.jpg 225w, https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-53-768x1024.jpg 768w, https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-53.jpg 960w\" width=\"225\"/>,\n",
       " <img alt=\"\" class=\"alignnone size-medium wp-image-9959\" decoding=\"async\" height=\"300\" sizes=\"(max-width: 225px) 100vw, 225px\" src=\"https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54-1-225x300.jpg\" srcset=\"https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54-1-225x300.jpg 225w, https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54-1-768x1024.jpg 768w, https://web.pharmacyboardkenya.org/wp-content/uploads/2024/07/PHOTO-2024-07-29-16-45-54-1.jpg 960w\" width=\"225\"/>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('div.entry-summary img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get the image caption\n",
    "soup.select('div.entry-summary figcaption')  #our image has no caption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting the url "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When downloading many HTML files, it is often difficult to find the original URLs of\n",
    "the files if they have not been saved separately, or the URLs might have changed.\n",
    "\n",
    "HTML tag called < link rel=\"canonical\">  can be used for this purpose. The tag\n",
    "is not mandatory, but it is extremely common, as it is also taken into account by\n",
    "search engines and contributes to a good ranking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://web.pharmacyboardkenya.org/pharmacy-and-poisons-board-adopts-new-technology-to-ensure-drug-quality-29th-july-2024/'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('link',{'rel':'canonical'})['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting list information (authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://health-data-commons.pharmaccess.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel Kapitan, Julie Fleischer, Chris Ihure, Rob Wiegman, Iris Bokkes, Mark van der Graaf'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find (\"meta\", {'name': 'author'})['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code returns only one author. If there is another author,\n",
    "which is unfortunately not contained in the meta-information of the page,\n",
    "it can be extracted again by selecting the elements in the browser and using the CSS\n",
    "selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://health-data-commons.pharmaccess.org/'\n",
    "\n",
    "#sel = \" \"  #replace this link with the html class where the authors are stored\n",
    "#soup.select(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming we got the name of the authors above and wanted to extract the name of the author in pure text:\n",
    "\n",
    "# [a.text for a in soup.select(sel)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semantic and nonsemantic content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sel selector is not semantic. Selection is performed\n",
    "based on layout-like classes.This works well for the moment but is likely to break if the layout is changed.\n",
    "\n",
    "Therefore, itâ€™s a good idea to avoid these kinds of selections\n",
    "if the code is likely to be executed not only once or in a batch but should also\n",
    "run in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting text of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Demonstrating fair sharing and reuse of health data in sub-Saharan Africa'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one('div.quarto-title p').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting reading time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select_one(\"p.ByLineBar_reading-time\").text \n",
    "# \n",
    "# #our article has no reading time in the source code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting attributes (IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import re \n",
    "\n",
    "def sanitize_file(file):\n",
    "    return re.sub(r'[^\\w\\-_\\.]','_', file)\n",
    "\n",
    "url = 'https://health-data-commons.pharmaccess.org/' \n",
    "\n",
    "s= requests.Session()\n",
    "#use the part after the lat / as the filename\n",
    "file = url.split('/')[-1] + '.html'\n",
    "file = sanitize_file(file)\n",
    "r = s.get(url)\n",
    "\n",
    "if r.ok:\n",
    "    with open(file, 'w+b') as f:\n",
    "        f.write(r.text.encode('utf-8'))\n",
    "\n",
    "else:\n",
    "    print(f'Error with URL: {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards a health data commons in LMICs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(file, 'r') as f:\n",
    "    html=f.read()\n",
    "    g = re.search(r'<title>(.*)</title>', html, re.MULTILINE | re.DOTALL)\n",
    "    if g:\n",
    "        print(g.groups()[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"unnumbered anchored\" data-anchor-id=\"data-commons-as-a-catalyst-for-achieving-uhc\">Data commons as a catalyst for achieving UHC</h2>,\n",
       " <h2 class=\"unnumbered anchored\" data-anchor-id=\"building-on-the-openhie-framework\">Building on the openHIE framework</h2>,\n",
       " <h2 class=\"unnumbered anchored\" data-anchor-id=\"demonstrators-built-for-momcare-project\">Demonstrators built for MomCare project</h2>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup.select('h2.unnumbered.anchored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select_one('div.quarto-title')['id']\n",
    "\n",
    "#our article has no ID that identifies it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the authors, the article carries more attributions. They can be found at the\n",
    "end of the text and reside in a special container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAyaz, Muhammad, Muhammad F Pasha, Mohammed Y Alzahrani, Rahmat Budiarto, and Deris Stiawan. 2021. Ã¢\\x80\\x9cThe Fast Health Interoperability Resources (FHIR) Standard: Systematic Literature Review of Implementations, Applications, Challenges and Opportunities.Ã¢\\x80\\x9d JMIR Medical Informatics 9 (7): e21929.\\n\\n\\nBeck, Micah. 2019. Ã¢\\x80\\x9cOn the Hourglass Model.Ã¢\\x80\\x9d Communications of the ACM 62 (7): 48Ã¢\\x80\\x9357.\\n\\n\\nDuda, Stephany N, Nan Kennedy, Douglas Conway, Alex C Cheng, Viet Nguyen, Teresa Zayas-CabÃƒÂ¡n, and Paul A Harris. 2022. Ã¢\\x80\\x9cHL7 FHIR-based Tools and Initiatives to Support Clinical Research: A Scoping Review.Ã¢\\x80\\x9d Journal of the American Medical Informatics Association 29 (9): 1642Ã¢\\x80\\x9353.\\n\\n\\nGebreslassie, Tesfit Gebremeskel, Mirjam van Reisen, Samson Yohannes Amare, Getu Tadele Taye, and Ruduan Plug. 2023. Ã¢\\x80\\x9cFHIR4FAIR: Leveraging FHIR in Health Data FAIRfication Process: In the Case of VODAN-A.Ã¢\\x80\\x9d FAIR Connect 1 (1): 49Ã¢\\x80\\x9354.\\n\\n\\nJones, James, Daniel Gottlieb, Joshua C Mandel, Vladimir Ignatov, Alyssa Ellis, Wayne Kubick, and Kenneth D Mandl. 2021. Ã¢\\x80\\x9cA Landscape Survey of Planned SMART/HL7 Bulk FHIR Data Access API Implementations and Tools.Ã¢\\x80\\x9d Journal of the American Medical Informatics Association 28 (6): 1284Ã¢\\x80\\x9387.\\n\\n\\nKickbusch, Ilona, Dario Piselli, Anurag Agrawal, Ran Balicer, Olivia Banner, Michael Adelhardt, Emanuele Capobianco, et al. 2021. Ã¢\\x80\\x9cThe Lancet and Financial Times Commission on Governing Health Futures 2030: Growing up in a Digital World.Ã¢\\x80\\x9d The Lancet 398 (10312): 1727Ã¢\\x80\\x9376.\\n\\n\\nMamuye, Adane L., Tesfahun M. Yilma, Ahmad Abdulwahab, Sean Broomhead, Phumzule Zondo, Mercy Kyeng, Justin Maeda, Mohammed Abdulaziz, Tadesse Wuhib, and Binyam C. Tilahun. 2022. Ã¢\\x80\\x9cHealth Information Exchange Policy and Standards for Digital Health Systems in Africa: A Systematic Review.Ã¢\\x80\\x9d PLOS Digital Health 1 (10): e0000118.\\n\\n\\nMandel, Joshua C, David A Kreda, Kenneth D Mandl, Isaac S Kohane, and Rachel B Ramoni. 2016. Ã¢\\x80\\x9cSMART on FHIR: A Standards-Based, Interoperable Apps Platform for Electronic Health Records.Ã¢\\x80\\x9d Journal of the American Medical Informatics Association 23 (5): 899Ã¢\\x80\\x93908.\\n\\n\\nMandl, Kenneth D., Daniel Gottlieb, Joshua C. Mandel, Vladimir Ignatov, Raheel Sayeed, Grahame Grieve, James Jones, Alyssa Ellis, and Adam Culbertson. 2020. Ã¢\\x80\\x9cPush Button Population Health: The SMART/HL7 FHIR Bulk Data Access Application Programming Interface.Ã¢\\x80\\x9d Npj Digital Medicine 3 (1): 1Ã¢\\x80\\x939.\\n\\n\\nMcCool, Judith, Rosie Dobson, Robyn Whittaker, and Chris Paton. 2022. Ã¢\\x80\\x9cMobile Health (mHealth) in Low- and Middle-Income Countries.Ã¢\\x80\\x9d Annual Review of Public Health 43 (1): 525Ã¢\\x80\\x9339.\\n\\n\\nNeumark, Tom, and Ruth J. Prince. 2021. Ã¢\\x80\\x9cDigital Health in East Africa: Innovation, Experimentation and the Market.Ã¢\\x80\\x9d Global Policy 12 (S6): 65Ã¢\\x80\\x9374.\\n\\n\\nRieke, Nicola, Jonny Hancox, Wenqi Li, Fausto MilletarÃƒÂ¬, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, et al. 2020. Ã¢\\x80\\x9cThe Future of Digital Health with Federated Learning.Ã¢\\x80\\x9d Npj Digital Medicine 3 (1): 1Ã¢\\x80\\x937.\\n\\n\\nSchultes, Erik. 2023. Ã¢\\x80\\x9cThe FAIR Hourglass: A Framework for FAIR Implementation.Ã¢\\x80\\x9d Edited by Barbara Magagna. FAIR Connect 1 (1): 13Ã¢\\x80\\x9317.\\n\\n\\nÃ¢\\x80\\x9cTanzania Health Enterprise Architecture.Ã¢\\x80\\x9d 2020.\\n\\n\\nvan Reisen, Mirjam, Francisca Oladipo, Mia Stokmans, Mouhamed Mpezamihgo, Sakinat Folorunso, Erik Schultes, Mariam Basajja, et al. 2021. Ã¢\\x80\\x9cDesign of a FAIR Digital Data Health Infrastructure in Africa for COVID-19 Reporting and Research.Ã¢\\x80\\x9d Advanced Genetics (Hoboken, N.j.) 2 (2): e10050.\\n\\n\\nVorisek, Carina Nina, Moritz Lehne, Sophie Anne Ines Klopfenstein, Paula Josephine Mayer, Alexander Bartschke, Thomas Haese, and Sylvia Thun. 2022. Ã¢\\x80\\x9cFast Healthcare Interoperability Resources (FHIR) for Interoperability in Health Research: Systematic Review.Ã¢\\x80\\x9d JMIR Medical Informatics 10 (7): e35724.\\n\\n\\nWorld Health Organization. 2021. Ã¢\\x80\\x9cGlobal Strategy on Digital Health 2020-2025.Ã¢\\x80\\x9d Geneva.\\n\\n\\nWorld Health Organization, and World Bank. 2021. Ã¢\\x80\\x9cTracking Universal Health Coverage: 2021 Global Monitoring Report.Ã¢\\x80\\x9d Washington, DC: World Health Organization.\\n\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one('div#refs.references.csl-bib-body.hanging-indent').text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is crucial to know the time that the article was posted.\n",
    "This is mentioned next to the section, but unfortunately it is constructed to be\n",
    "human-readable (like â€œ3 days agoâ€). This can be parsed but is tedious. Knowing the\n",
    "real publishing time, the correct element can be found in the HTML head element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptime = soup.find('meta', {'property': 'article:published_time'})['content'] \n",
    "\n",
    "#our article has no published time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the string time output to a datetime object\n",
    "\n",
    "# from dateutil import parser\n",
    "\n",
    "# parser.parse(ptime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "Use regular expressions only for crude extraction. An HTML parser is slower but\n",
    "much easier to use and more stable\n",
    "\n",
    "It often makes sense to take a look at the semantic structure of the documents and\n",
    "use HTML tags that have semantic class names to find the value of structural elements.\n",
    "These tags have the advantage that they are the same over a large class of web\n",
    "pages. Extraction of their content therefore has to be implemented only once and can\n",
    "be reused.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Spidering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from dateutil import parser \n",
    "\n",
    "#1. Define how many pages of the archive should be downloaded\n",
    "def download_archive (page):\n",
    "    filename = 'page-%06d.html' % page #filename = 'page-000001.html'\n",
    "    if not os.path.isfile(filename):\n",
    "        url = \"https://www.reuters.com/news/archive/\" + \\\n",
    "              \"?view=page&page=%d&pageSize=10\" % page \n",
    "        r = requests.get(url)\n",
    "        with open(filename, 'w+') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "#2. Download each page of the archive into a file and for each page extract the links to the articles\n",
    "def parse_archive_page(page_file):\n",
    "    with open(page_file, 'r') as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    hrefs = ['https://www.reuters.com' + a['href']\n",
    "             for a in soup.select('article.story div.story-content a')]\n",
    "    \n",
    "    return hrefs\n",
    "#4. For each article url download it to a html file, if the article file is present, skip this step\n",
    "def download_article(url):\n",
    "    #check if article is already present\n",
    "    filename = url.split('/')[-1] + '.html'\n",
    "    if not os.path.isfile(filename):\n",
    "        r = requests.get(url)\n",
    "        with open(filename, 'w+') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "#5. For each article file, extract the content into a dict and combine the dicts to a dataframe\n",
    "def parse_article(article_file):\n",
    "    def find_obfuscated_class(soup, klass):\n",
    "        return soup.find_all(lambda tag: tag.has_attr('class') and (klass in \" \".join(tag['class'])))\n",
    "    with open (article_file, 'r') as f:\n",
    "        html = f.read()\n",
    "    r = {}  #initate an empty dict\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    r['id'] = soup.select_one('div.StandardArticle_inner-container')['id']\n",
    "    r['url'] = soup.find('link', {'rel': 'canonical'})['href']\n",
    "    r['headline'] = soup.h1.text\n",
    "    r['section'] = find_obfuscated_class(soup, \"ArticleHeader-channel\")[0].text\n",
    "    \n",
    "    r['text'] = \"\\n\".join([t.text for t in find_obfuscated_class(soup, \"Paragraph-paragraph\")])\n",
    "    r['authors'] = find_obfuscated_class(soup, \"Attribution-attribution\")[0].text\n",
    "    r['time'] = soup.find('meta', {'property':\n",
    "                                   'og:article:published_time'})['content']\n",
    "    \n",
    "    return r \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download 10 pages of the archive\n",
    "for p in range(1, 10):\n",
    "    download_archive(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass archive and add to article_urls\n",
    "import glob \n",
    "\n",
    "article_urls = []\n",
    "for page_file in glob.glob('page-*.html'):\n",
    "    article_urls += parse_archive_page(page_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download articles \n",
    "for url in article_urls:\n",
    "    download_article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'time' column is not present in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#arrange in pandas DataFrame\n",
    "import pandas as pd\n",
    "import glob \n",
    "\n",
    "df = pd.DataFrame() \n",
    "for article_file in glob.glob(\"*-id???????????.html\"):\n",
    "    df = df.append(parse_article(article_file), ignore_index=True)  # Fixed indentation\n",
    "\n",
    "if 'time' in df.columns:\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "else:\n",
    "    print(\"The 'time' column is not present in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density-Based Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[no-title]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from readability import Document \n",
    "doc = Document('https://web.pharmacyboardkenya.org/feed/')  #replace the url with a html page, use requests lib to get the html\n",
    "doc.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc.short_title()\n",
    "\n",
    "#finding the title or summary of the page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to extract the body part of the article\n",
    "\n",
    "#density_soup = BeautifulSoup(html, 'html.parser')\n",
    "#density_soup.body.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the cases, python-readability works\n",
    "reasonably well and removes the need to implement too many special cases. However,\n",
    "the cost of using this library is uncertainty.\n",
    "\n",
    " Will it always work in the expected way\n",
    "with the impossibility of extracting structured data such as timestamps, authors, and\n",
    "so on (although there might be other heuristics for that)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blueprint: Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the code for scrapy cannot be changed easily. One more argument for using up to date separate libraries. In the version used here, it still collects the titles of the articles but not more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import logging \n",
    "\n",
    "class ReutersArchiveSpider(scrapy.Spider):\n",
    "    name = 'reuters_archive'\n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'FEED_FORMAT': 'json',\n",
    "        'FEED_URL': 'reuters_archive.json'\n",
    "    }\n",
    "\n",
    "    start_urls = ['https://www.reuters.com/news/archive/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for article in response.css('article.story div.story-content a'):\n",
    "            yield response.follow(article.css('a::attr(href)').extract_first(), self.parse_article)\n",
    "\n",
    "        next_page_url = response.css('a.control-nav-next::attr(href)').extract_first()\n",
    "        if (next_page_url is not None) & ('page=2' not in next_page_url):\n",
    "            yield response.follow(next_page_url, self.parse)\n",
    "\n",
    "    def parse_article(self, response):\n",
    "        yield {\n",
    "            'title': response.css('h1::text').extract_first().strip(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy works in an object-oriented way. For each so-called spider, a class needs to be\n",
    "implemented that is derived from scrapy.Spider. Scrapy adds a lot of debug output,\n",
    "which we reduce by logging.WARNING. \n",
    "\n",
    "The base class automatically\n",
    "calls the parse function with the start_urls. This function extracts the\n",
    "links to the article and invokes yield with the function parse_article as a parameter.\n",
    "\n",
    "This function in turn extracts some attributes from the articles and yields them\n",
    "in a dict. Finally, the next page link is crawled, but we stop here before getting the\n",
    "second page.\n",
    "\n",
    "yield has a double functionality in Scrapy. If a dict is yielded, it is added to the\n",
    "results. If a Request object is yielded, the object is fetched and gets parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 13:06:00 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-09-30 13:06:00 [scrapy.utils.log] INFO: Versions: lxml 5.2.1.0, libxml2 2.10.4, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.0.0 (OpenSSL 3.0.14 4 Jun 2024), cryptography 42.0.5, Platform Windows-11-10.0.22631-SP0\n",
      "2024-09-30 13:06:00 [py.warnings] WARNING: c:\\Users\\user\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this can be run only once from a Jupyter notebook due to Twisted dependencies which are ancient \n",
    "from scrapy.crawler import CrawlerProcess \n",
    "process = CrawlerProcess()\n",
    "\n",
    "process.crawl(ReutersArchiveSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('*.json') #return a list of paths matching a pathname pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat 'reuters_archive.json'  #return the content of a file - in our case, it's nothing since an empty list was returned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "\n",
    "1. As most of the coding is spent in extracting data in the articles, this code has to\n",
    "change frequently. For this, spidering has to be restarted (and if you are running\n",
    "the script in Jupyter, you also have to start the Jupyter notebook server), which\n",
    "tremendously increases turnaround times.\n",
    "\n",
    "2. Itâ€™s nice that JSON can directly be produced. Be careful as the JSON file is\n",
    "appended, which can result in an invalid JSON if you donâ€™t delete the file before\n",
    "starting the spidering process. This can be solved by using the so-called jl format\n",
    "(JSON lines), but it is a workaround\n",
    "\n",
    "3. Scrapy has some nice ideas. In our day-to-day work, we do not use it, mainly\n",
    "because debugging is hard. If persistence of the HTML files is needed (which is\n",
    "strongly recommended), it loses lots of advantages. The object-oriented approach is useful\n",
    "and can be implemented outside of Scrapy without too much effort\n",
    "\n",
    "4. For some\n",
    "websites, ready-made Scrapy spiders might already be available and can be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
